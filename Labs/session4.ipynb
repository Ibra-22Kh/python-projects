{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 - import Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.datasets import make_moons\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2- Prepare Classification Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create artificial dataset (classification problem within 2 classes within R^2 input space)\n",
    "X, y = make_moons(n_samples=900, noise=0.2, random_state=0)\n",
    "\n",
    "# Preprocess dataset, and split into training and test part\n",
    "X = StandardScaler().fit_transform(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.7)\n",
    "\n",
    "# Encode class labels as binary vector (with exactly ONE bit set to 1, and all others to 0)\n",
    "Y_train_OneHot = np.eye(2)[y_train]\n",
    "Y_test_OneHot = np.eye(2)[y_test]\n",
    "\n",
    "# Print beginning of training dataset (for verification)\n",
    "print(\"Number of training examples = \", y_train.size)\n",
    "print()\n",
    "print(\"  first \", round(y_train.size/10), \"training examples\" )\n",
    "print(\"[  Input_features  ]     [Target_output]\")\n",
    "for i in range( int(round(y_train.size/10) )):\n",
    "    print( X_train[i], Y_train_OneHot[i])\n",
    "\n",
    "# Plot training+testing dataset\n",
    "################################\n",
    "cm = plt.cm.RdBu\n",
    "cm_bright = ListedColormap(['#FF0000', '#0000FF'])\n",
    "\n",
    "# Plot the training points...\n",
    "plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright)\n",
    "#   ...and testing points\n",
    "plt.scatter(X_test[:, 0], X_test[:, 1], marker='x', c=y_test, cmap=cm_bright, alpha=0.3)\n",
    "\n",
    "# Define limits/scale of plot axis\n",
    "x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "plt.xlim(x_min, x_max)\n",
    "plt.ylim(y_min, y_max)\n",
    "\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "\n",
    "# Actually render the plot\n",
    "print()\n",
    "print(\"PLOT OF TRAINING EXAMPLES AND TEST DATASET\")\n",
    "print(\"Datasets: circles=training, light-crosses=test [and red=class_1, blue=class_2]\")\n",
    "\n",
    "plt.ioff()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 - Create and parametrize a MLP neural network classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#########################################################\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "clf = MLPClassifier(hidden_layer_sizes=(5, ), activation='tanh', solver='sgd', \n",
    "                    alpha=0.0000001, batch_size=4,learning_rate='constant', learning_rate_init=0.005, \n",
    "                    power_t=0.5, max_iter=500, shuffle=True, random_state=11, tol=0.00001, \n",
    "                    verbose=True, warm_start=False, momentum=0.2, nesterovs_momentum=True, \n",
    "                    early_stopping=False, validation_fraction=0.2, \n",
    "                    beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "\n",
    "###\n",
    "# Parameters\n",
    "# hidden_layer_sizestuple, length = n_layers - 2, default=(100,)\n",
    "# The ith element represents the number of neurons in the ith hidden layer.\n",
    "\n",
    "# activation{‘identity’, ‘logistic’, ‘tanh’, ‘relu’}, default=’relu’\n",
    "# Activation function for the hidden layer.\n",
    "\n",
    "# ‘identity’, no-op activation, useful to implement linear bottleneck, returns f(x) = x\n",
    "\n",
    "# ‘logistic’, the logistic sigmoid function, returns f(x) = 1 / (1 + exp(-x)).\n",
    "\n",
    "# ‘tanh’, the hyperbolic tan function, returns f(x) = tanh(x).\n",
    "\n",
    "# ‘relu’, the rectified linear unit function, returns f(x) = max(0, x)\n",
    "\n",
    "# solver{‘lbfgs’, ‘sgd’, ‘adam’}, default=’adam’\n",
    "# The solver for weight optimization.\n",
    "\n",
    "# ‘lbfgs’ is an optimizer in the family of quasi-Newton methods.\n",
    "\n",
    "# ‘sgd’ refers to stochastic gradient descent.\n",
    "\n",
    "# ‘adam’ refers to a stochastic gradient-based optimizer proposed by Kingma, Diederik, and Jimmy Ba\n",
    "\n",
    "# Note: The default solver ‘adam’ works pretty well on relatively large datasets (with thousands of training samples or more) in terms of both training time and validation score. For small datasets, however, ‘lbfgs’ can converge faster and perform better.\n",
    "\n",
    "\n",
    "# learning_rate{‘constant’, ‘invscaling’, ‘adaptive’}, default=’constant’\n",
    "# Learning rate schedule for weight updates.\n",
    "\n",
    "# ‘constant’ is a constant learning rate given by ‘learning_rate_init’.\n",
    "\n",
    "# ‘invscaling’ gradually decreases the learning rate learning_rate_ at each time step ‘t’ using an inverse scaling exponent of ‘power_t’. effective_learning_rate = learning_rate_init / pow(t, power_t)\n",
    "\n",
    "# ‘adaptive’ keeps the learning rate constant to ‘learning_rate_init’ as long as training loss keeps decreasing. Each time two consecutive epochs fail to decrease training loss by at least tol, or fail to increase validation score by at least tol if ‘early_stopping’ is on, the current learning rate is divided by 5.\n",
    "\n",
    "# Only used when solver=’sgd’.\n",
    "\n",
    "# learning_rate_initdouble, default=0.001\n",
    "# The initial learning rate used. It controls the step-size in updating the weights. Only used when solver=’sgd’ or ‘adam’.\n",
    "\n",
    "\n",
    "# max_iterint, default=200\n",
    "# Maximum number of iterations. The solver iterates until convergence (determined by ‘tol’) or this number of iterations. For stochastic solvers (‘sgd’, ‘adam’), note that this determines the number of epochs (how many times each data point will be used), not the number of gradient steps.\n",
    "\n",
    "# momentumfloat, default=0.9\n",
    "# Momentum for gradient descent update. Should be between 0 and 1. Only used when solver=’sgd’.\n",
    "\n",
    "# nesterovs_momentumboolean, default=True\n",
    "# Whether to use Nesterov’s momentum. Only used when solver=’sgd’ and momentum > 0.\n",
    "\n",
    "\n",
    "# n_iter_no_changeint, default=10\n",
    "# Maximum number of epochs to not meet tol improvement. Only effective when solver=’sgd’ or ‘adam’\n",
    "\n",
    "\n",
    "\n",
    "print(clf)\n",
    "\n",
    "# Train the MLP classifier on training dataset\n",
    "clf.fit(X_train, Y_train_OneHot)\n",
    "print()\n",
    "\n",
    "# Evaluate acuracy on test data\n",
    "score = clf.score(X_test,Y_test_OneHot)\n",
    "print(\"Acuracy (on test set) = \", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot the decision boundary. For that, we will assign a color to each\n",
    "#   point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "\n",
    "h = .02  # Step size in the mesh\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "\n",
    "# Compute class probabilities for each mesh point\n",
    "Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n",
    "\n",
    "# Put the result into a color plot\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.contourf(xx, yy, Z, cmap=cm, alpha=.8)\n",
    "\n",
    "# Plot also the training points\n",
    "plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright)\n",
    "# and testing points\n",
    "plt.scatter(X_test[:, 0], X_test[:, 1], marker='x', c=y_test, cmap=cm_bright, alpha=0.3)\n",
    "\n",
    "# Axis ranges \n",
    "plt.xlim(xx.min(), xx.max())\n",
    "plt.ylim(yy.min(), yy.max())\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "\n",
    "# Print acuracy on plot\n",
    "plt.text(xx.max() - .3, yy.min() + .3, ('%.2f' % score).lstrip('0'),\n",
    "                size=15, horizontalalignment='right')\n",
    "\n",
    "# Actually plot\n",
    "plt.ioff()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4- your turn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here your answer \n",
    "# first momentom = 0  \n",
    "# then use diffrent momentums as  [0, .9] and compare results\n",
    "# try diffrent solvers and compare your answers  (SGD, ADAM )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5- grid search "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "param_grid = [\n",
    "  {'hidden_layer_sizes': [(5,), (10,), (15,), (25,)], \n",
    "   'learning_rate_init':[0.003, 0.01, 0.03, 0.1],\n",
    "   'alpha': [0.00001, 0.0001, 0.001, 0.01]}\n",
    " ]\n",
    "#print(param_grid)\n",
    "\n",
    "# Cross-validation grid-search\n",
    "scores = ['precision', 'recall']\n",
    "for score in scores:\n",
    "    clf = GridSearchCV( MLPClassifier(activation='tanh', alpha=1e-07, batch_size=4, beta_1=0.9,\n",
    "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
    "       hidden_layer_sizes=(10,), learning_rate='constant',\n",
    "       learning_rate_init=0.005, max_iter=500, momentum=0.8,\n",
    "       nesterovs_momentum=True, power_t=0.5, random_state=11, shuffle=True,\n",
    "       solver='adam', tol=1e-05, validation_fraction=0.3, verbose=False,\n",
    "       warm_start=False), \n",
    "       param_grid, cv=3, scoring='%s_macro' % score)\n",
    "    \n",
    "    clf.fit(X_train, Y_train_OneHot)\n",
    "    print(\"Best parameters set found on development set:\")\n",
    "    print()\n",
    "    print(clf.best_params_)\n",
    "    print()\n",
    "    print(\"Grid scores on development set:\")\n",
    "    print()\n",
    "    means = clf.cv_results_['mean_test_score']\n",
    "    stds = clf.cv_results_['std_test_score']\n",
    "    for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "           % (mean, std * 2, params))\n",
    "    print()\n",
    "    print()\n",
    "    print(\"The model is trained on the full development set.\")\n",
    "    print(\"The scores are computed on the full evaluation set.\")\n",
    "    print()\n",
    "    y_true, y_pred = Y_test_OneHot, clf.predict(X_test)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the decision boundary. For that, we will assign a color to each\n",
    "#   point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "\n",
    "h = .02  # Step size in the mesh\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                        np.arange(y_min, y_max, h))\n",
    "\n",
    "# Compute class probabilities for each mesh point\n",
    "Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n",
    "\n",
    "# Put the result into a color plot\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.contourf(xx, yy, Z, cmap=cm, alpha=.8)\n",
    "\n",
    "# Plot also the training points\n",
    "plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright)\n",
    "# and testing points\n",
    "plt.scatter(X_test[:, 0], X_test[:, 1], marker='x', c=y_test, cmap=cm_bright, alpha=0.3)\n",
    "\n",
    "# Axis ranges \n",
    "plt.xlim(xx.min(), xx.max())\n",
    "plt.ylim(yy.min(), yy.max())\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "\n",
    "\n",
    "# Actually plot\n",
    "plt.ioff()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6- regression problem \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the random seed for reproducible results\n",
    "np.random.seed(42)\n",
    "\n",
    "# \"True\" generating function representing a process in real life\n",
    "def true_gen(x):\n",
    "    y = np.sin(1.2 * x * np.pi) \n",
    "    return(y)\n",
    "\n",
    "# x values and y value with a small amount of random noise\n",
    "x = np.sort(np.random.rand(120))\n",
    "y = true_gen(x) + 0.1 * np.random.randn(len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x_train,x_test,y_train,y_test = train_test_split(x,y)\n",
    "\n",
    "x_train.resize(x_train.shape[0], 1)\n",
    "x_test.resize(x_test.shape[0], 1)\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "# Model the true curve\n",
    "x_linspace = np.linspace(0, 1, 1000)\n",
    "y_true = true_gen(x_linspace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize observations and true curve\n",
    "plt.plot(x_train, y_train, 'k.', label = 'Train'); \n",
    "plt.plot(x_test, y_test, 'r.', label = 'Test')\n",
    "plt.plot(x_linspace, y_true, 'b-', linewidth = 2, label = 'True Function')\n",
    "plt.legend()\n",
    "plt.xlabel('x'); plt.ylabel('y'); plt.title('Data');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7- your turn again :\n",
    "\n",
    "    1- use MLPregressor to aproximate this function\n",
    "    2- redo the steps asked in 4:\n",
    "        use diffrent momentums as  [0, .9] and compare results\n",
    "        try diffrent solvers and compare your answers  (SGD, ADAM )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "help(MLPRegressor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "clf = MLPRegressor(hidden_layer_sizes=(5, ), activation='tanh', solver='adam', \n",
    "                    alpha=0.0000001, batch_size=4,learning_rate='constant', learning_rate_init=0.001, \n",
    "                    power_t=0.5, max_iter=500, shuffle=True, random_state=11, tol=0.00001, \n",
    "                    verbose=True, warm_start=False, momentum=0.9, nesterovs_momentum=True, \n",
    "                    early_stopping=False, validation_fraction=0.2, \n",
    "                    beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "\n",
    "print(clf)\n",
    "\n",
    "# Train the MLP classifier on training dataset\n",
    "clf.fit(x_train, y_train)\n",
    "print()\n",
    "\n",
    "# Evaluate acuracy on test data\n",
    "score = clf.score(x_test,y_test)\n",
    "print(\"Acuracy (on test set) = \", score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
